{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1808b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all_ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f83c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "844dd00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d84960",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d301383",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cede5992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user is asking for the capital of Indonesia. Let me recall. I think Indonesia has a capital, but I might be mixing it up with another country. Wait, Jakarta was the capital, right? But I remember hearing something about a move to a new city. Let me verify.\\n\\nIndonesia is an archipelago in Southeast Asia, right? They used to have Jakarta as their capital, which is on the island of Java. But I think there was a decision to relocate the capital. Was it to Kalimantan? Or maybe another island? I think the new capital is called something like Nusantara. Let me check my memory.\\n\\nYes, in 2019, the Indonesian government announced plans to move the capital from Jakarta to East Kalimantan on the island of Borneo. The new capital is being developed in a place called Nusantara. The move is supposed to alleviate the overpopulation and traffic issues in Jakarta and to promote more balanced development across the country. However, the process might be ongoing, and Jakarta might still be considered the de facto capital until the transition is complete.\\n\\nI should also mention that there might be some confusion because sometimes people refer to Jakarta as the capital even though the official move is underway. It\\'s important to clarify both the current status and the planned change. Let me make sure about the timeline. The new capital was supposed to start transferring in 2022, but it might have been delayed. So the answer would be that the capital is transitioning from Jakarta to Nusantara in Kalimantan. But perhaps the user just needs the current answer. Wait, the question is \"What is the capital of Indonesia?\" So if the move isn\\'t fully completed, technically the capital is still Jakarta, but the government is relocating it. Alternatively, in official terms, maybe they already consider it the new capital. I need to confirm.\\n\\nLooking up the latest information: As of now, the Indonesian government has officially designated Nusantara as the new capital, but the relocation process is ongoing. However, Jakarta remains the de facto capital until the transition is complete. So the answer should state that the capital is in the process of being moved from Jakarta to Nusantara. But perhaps the user is looking for the current recognized capital, which is Jakarta. I need to present both facts clearly without assuming too much.\\n</think>\\n\\nThe capital of Indonesia is currently in the process of being relocated. **Jakarta** has historically been the capital, but the Indonesian government has officially designated **Nusantara** (located in East Kalimantan, on the island of Borneo) as the new capital. \\n\\nThe relocation process began in 2022, aiming to alleviate Jakarta\\'s urban challenges (e.g., overpopulation, traffic, and sinking land) and promote balanced national development. While administrative functions are gradually shifting to Nusantara, **Jakarta remains the de facto capital** until the transition is fully completed. \\n\\nFor the most up-to-date status, check recent government updates, as the timeline may evolve.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of Indonesia?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2933308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4ba9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15c5fa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04257791489362717,\n",
       " -0.047810737043619156,\n",
       " -0.02702580951154232,\n",
       " -0.035097863525152206,\n",
       " 0.05324113741517067,\n",
       " 0.0018493696115911007,\n",
       " 0.004823467694222927,\n",
       " -0.022051338106393814,\n",
       " 0.0009697225177660584,\n",
       " 0.07324519753456116,\n",
       " -0.014812891371548176,\n",
       " 0.003644853364676237,\n",
       " -0.00034491211408749223,\n",
       " 0.028128888458013535,\n",
       " 0.025020018219947815,\n",
       " -0.04156218096613884,\n",
       " 0.005471833515912294,\n",
       " 0.02652869001030922,\n",
       " 0.043672770261764526,\n",
       " -0.014782802201807499,\n",
       " 0.013127882033586502,\n",
       " 0.007567551918327808,\n",
       " -0.03469207510352135,\n",
       " 0.023462051525712013,\n",
       " 0.020962651818990707,\n",
       " -0.05559537559747696,\n",
       " 0.00859882216900587,\n",
       " -0.04824773594737053,\n",
       " -0.012368501164019108,\n",
       " -0.001532181748189032,\n",
       " -0.07255345582962036,\n",
       " 0.04269229248166084,\n",
       " 0.00527808116748929,\n",
       " -0.015593086369335651,\n",
       " 0.02645784802734852,\n",
       " -0.0531519278883934,\n",
       " -0.0004922056687064469,\n",
       " 0.016876710578799248,\n",
       " -0.00814562477171421,\n",
       " 0.04225021228194237,\n",
       " -0.015036126598715782,\n",
       " -0.003916633781045675,\n",
       " -0.04687097668647766,\n",
       " 0.015219401568174362,\n",
       " -0.009408559650182724,\n",
       " -0.01825689896941185,\n",
       " -0.01993844285607338,\n",
       " 0.0748581662774086,\n",
       " 0.019254358485341072,\n",
       " -0.0052777305245399475,\n",
       " 0.012134595774114132,\n",
       " -0.010617725551128387,\n",
       " 0.054592013359069824,\n",
       " 0.020773116499185562,\n",
       " 0.013602628372609615,\n",
       " -0.06971295922994614,\n",
       " 0.008997276425361633,\n",
       " -0.014119189232587814,\n",
       " -0.0046128276735544205,\n",
       " 0.0210944302380085,\n",
       " 0.0329081267118454,\n",
       " -0.030065499246120453,\n",
       " 0.00447330716997385,\n",
       " 0.0407467782497406,\n",
       " 0.01971225067973137,\n",
       " -0.055106159299612045,\n",
       " 0.03556030988693237,\n",
       " 0.013304406777024269,\n",
       " 0.08534496277570724,\n",
       " 0.00158949033357203,\n",
       " -0.004593267105519772,\n",
       " -0.042083490639925,\n",
       " 0.0713706836104393,\n",
       " 0.005657872185111046,\n",
       " 0.030689362436532974,\n",
       " -0.0817095935344696,\n",
       " -0.02232016995549202,\n",
       " 0.06454379111528397,\n",
       " 0.01360271591693163,\n",
       " -0.04246792569756508,\n",
       " -0.00804409384727478,\n",
       " -0.05186443775892258,\n",
       " -0.07956809550523758,\n",
       " -0.02714453637599945,\n",
       " -0.05819103121757507,\n",
       " 0.014083041809499264,\n",
       " -0.05621412768959999,\n",
       " -0.018843192607164383,\n",
       " -0.05411393195390701,\n",
       " 0.04985203966498375,\n",
       " -0.025272108614444733,\n",
       " 0.006586411036550999,\n",
       " 0.07480309158563614,\n",
       " -0.040020957589149475,\n",
       " -0.028987407684326172,\n",
       " 0.06478418409824371,\n",
       " -0.014494857750833035,\n",
       " -0.011784432455897331,\n",
       " 0.07746846228837967,\n",
       " -0.003581888973712921,\n",
       " 0.01965465024113655,\n",
       " -0.021930739283561707,\n",
       " -0.07060352712869644,\n",
       " 0.05417868122458458,\n",
       " 0.03164941444993019,\n",
       " -0.00866173766553402,\n",
       " -0.01214554999023676,\n",
       " 0.058456458151340485,\n",
       " 0.006299072410911322,\n",
       " 0.06679613888263702,\n",
       " -0.0667213425040245,\n",
       " -0.04552998021245003,\n",
       " 0.031006908044219017,\n",
       " 0.06095348298549652,\n",
       " 0.01496248971670866,\n",
       " -0.05309277027845383,\n",
       " -0.03518706187605858,\n",
       " 0.017601503059267998,\n",
       " 0.03776673972606659,\n",
       " 0.024583736434578896,\n",
       " 0.035969726741313934,\n",
       " -0.018826857209205627,\n",
       " 0.044718313962221146,\n",
       " -0.045678626745939255,\n",
       " 0.034775298088788986,\n",
       " -0.01982470043003559,\n",
       " -0.04870112985372543,\n",
       " 0.032311610877513885,\n",
       " -0.011565012857317924,\n",
       " 0.021644998341798782,\n",
       " 0.015746233984827995,\n",
       " -0.04829183965921402,\n",
       " -0.014721618965268135,\n",
       " -0.002236217027530074,\n",
       " 0.014409353025257587,\n",
       " 0.030544603243470192,\n",
       " 0.061715949326753616,\n",
       " -0.018756143748760223,\n",
       " 0.04099138453602791,\n",
       " 0.006017507519572973,\n",
       " 0.029441386461257935,\n",
       " 0.0671597346663475,\n",
       " 0.0025576308835297823,\n",
       " 0.028602181002497673,\n",
       " -0.018605684861540794,\n",
       " 0.05013180524110794,\n",
       " -0.0542902871966362,\n",
       " -0.03664889186620712,\n",
       " 0.040750712156295776,\n",
       " -0.044509004801511765,\n",
       " -0.07576945424079895,\n",
       " 0.0016938719199970365,\n",
       " -0.04662192612886429,\n",
       " -0.02760942094027996,\n",
       " 0.0392257384955883,\n",
       " 0.010924643836915493,\n",
       " -0.0143031757324934,\n",
       " 0.048677846789360046,\n",
       " 0.01945548690855503,\n",
       " -0.0165046788752079,\n",
       " 0.05750593915581703,\n",
       " 0.017886588349938393,\n",
       " 0.016367116943001747,\n",
       " 0.006838107947260141,\n",
       " -0.0027852400671690702,\n",
       " -0.028113022446632385,\n",
       " 0.03293533995747566,\n",
       " -0.008162261918187141,\n",
       " 0.016503559425473213,\n",
       " -0.0008890617173165083,\n",
       " -0.011331772431731224,\n",
       " 0.01560590323060751,\n",
       " -0.002747876103967428,\n",
       " -0.023822380229830742,\n",
       " 0.008317090570926666,\n",
       " -0.033890966325998306,\n",
       " 0.011247474700212479,\n",
       " -0.02326224185526371,\n",
       " -0.014204729348421097,\n",
       " -0.031394872814416885,\n",
       " 0.006071117240935564,\n",
       " -0.03975209593772888,\n",
       " 0.006350292824208736,\n",
       " 0.03685023635625839,\n",
       " 0.011773370206356049,\n",
       " -0.043271128088235855,\n",
       " 0.022107595577836037,\n",
       " -0.03222227841615677,\n",
       " 0.004633280448615551,\n",
       " 0.01861286163330078,\n",
       " -0.0461781807243824,\n",
       " -0.013411097228527069,\n",
       " -0.014593689702451229,\n",
       " -0.017189396545290947,\n",
       " -0.037615299224853516,\n",
       " -0.01175762340426445,\n",
       " 0.029062220826745033,\n",
       " -0.017437946051359177,\n",
       " 0.0454525351524353,\n",
       " -0.05209745094180107,\n",
       " -0.029428549110889435,\n",
       " 0.03321756049990654,\n",
       " 0.02435440942645073,\n",
       " -0.0358048640191555,\n",
       " 0.022126683965325356,\n",
       " -0.00966416671872139,\n",
       " 0.08779031038284302,\n",
       " -0.03464784100651741,\n",
       " -0.043734170496463776,\n",
       " 0.05736429989337921,\n",
       " -0.014013061299920082,\n",
       " 0.006000404711812735,\n",
       " -0.02421995811164379,\n",
       " 0.016350895166397095,\n",
       " 0.044808026403188705,\n",
       " -0.025562455877661705,\n",
       " 0.07690118998289108,\n",
       " 0.010970678180456161,\n",
       " 0.04207293689250946,\n",
       " -0.022715603932738304,\n",
       " -0.04467586800456047,\n",
       " -0.003055560402572155,\n",
       " -0.023600663989782333,\n",
       " -0.013115497305989265,\n",
       " 0.04398322477936745,\n",
       " 0.023938285186886787,\n",
       " -0.011075073853135109,\n",
       " -0.03778417780995369,\n",
       " 0.00021477344853337854,\n",
       " -0.010648282244801521,\n",
       " -0.05003296211361885,\n",
       " 0.07658620178699493,\n",
       " 0.03315397724509239,\n",
       " -0.02210102789103985,\n",
       " 0.05767066404223442,\n",
       " 0.0006767681916244328,\n",
       " -0.003242972306907177,\n",
       " 0.020267846062779427,\n",
       " 0.0006321387481875718,\n",
       " 0.0009972446132451296,\n",
       " -0.02512633241713047,\n",
       " -0.03888467326760292,\n",
       " 0.04662318155169487,\n",
       " 0.023061562329530716,\n",
       " -0.04041805863380432,\n",
       " -0.038651108741760254,\n",
       " -0.027900371700525284,\n",
       " 0.024071509018540382,\n",
       " 0.05060867220163345,\n",
       " 0.05143703520298004,\n",
       " -0.002084001898765564,\n",
       " 0.0024474747478961945,\n",
       " 0.032136015594005585,\n",
       " 0.019393740221858025,\n",
       " -0.07943607866764069,\n",
       " 0.02427038736641407,\n",
       " -0.06520397216081619,\n",
       " 0.031165381893515587,\n",
       " -0.035928234457969666,\n",
       " 0.03409767523407936,\n",
       " 0.030861197039484978,\n",
       " 0.03538651764392853,\n",
       " 0.042785514146089554,\n",
       " -0.03435273468494415,\n",
       " -0.015017978847026825,\n",
       " -0.03510842099785805,\n",
       " 0.006179507356137037,\n",
       " -0.05111760273575783,\n",
       " 0.012249006889760494,\n",
       " 0.005350593011826277,\n",
       " 0.03790914639830589,\n",
       " -0.07914953678846359,\n",
       " 0.017817426472902298,\n",
       " 0.0395965501666069,\n",
       " 0.036239635199308395,\n",
       " 0.012556263245642185,\n",
       " -0.050666097551584244,\n",
       " 0.057882554829120636,\n",
       " 0.06475129723548889,\n",
       " -0.0725456178188324,\n",
       " 0.03519894927740097,\n",
       " 0.030570028349757195,\n",
       " -0.006167229264974594,\n",
       " -0.011796296574175358,\n",
       " -0.008290175348520279,\n",
       " -0.017933540046215057,\n",
       " -0.04209362342953682,\n",
       " -0.012064228765666485,\n",
       " 0.00880552176386118,\n",
       " -0.05999321490526199,\n",
       " -0.03464368358254433,\n",
       " -0.08186905086040497,\n",
       " 0.0355706512928009,\n",
       " -0.04216013848781586,\n",
       " -0.10391668230295181,\n",
       " 0.0038084639236330986,\n",
       " -0.030506454408168793,\n",
       " 0.0399044007062912,\n",
       " 0.03539436310529709,\n",
       " -0.01851348765194416,\n",
       " -0.030506830662488937,\n",
       " 0.01296560000628233,\n",
       " 0.02962312288582325,\n",
       " -0.05594923719763756,\n",
       " 0.026137271896004677,\n",
       " 0.027979889884591103,\n",
       " -0.032783620059490204,\n",
       " -0.05861344188451767,\n",
       " 0.029305243864655495,\n",
       " 0.025631634518504143,\n",
       " 0.046621762216091156,\n",
       " -0.01578911766409874,\n",
       " -0.05855393409729004,\n",
       " -0.033567510545253754,\n",
       " -0.022099914029240608,\n",
       " 0.07099347561597824,\n",
       " -0.015482406131923199,\n",
       " -0.02221505530178547,\n",
       " 0.020550455898046494,\n",
       " 0.03193112462759018,\n",
       " 0.009649628773331642,\n",
       " 0.08112385869026184,\n",
       " 0.024486811831593513,\n",
       " -0.020967384800314903,\n",
       " -0.001703555346466601,\n",
       " 0.003984694369137287,\n",
       " 0.01214590948075056,\n",
       " 0.02251713164150715,\n",
       " 0.011797886341810226,\n",
       " -0.014598767273128033,\n",
       " -0.027774184942245483,\n",
       " 0.035769712179899216,\n",
       " -0.04636594280600548,\n",
       " 0.006575292441993952,\n",
       " 0.01091056503355503,\n",
       " 0.06001884862780571,\n",
       " -0.03792550787329674,\n",
       " 0.025552064180374146,\n",
       " -0.052944615483284,\n",
       " -0.00331985205411911,\n",
       " 0.04723644629120827,\n",
       " 0.04916655644774437,\n",
       " -0.012118092738091946,\n",
       " -0.015976974740624428,\n",
       " -0.023780548945069313,\n",
       " -0.020015906542539597,\n",
       " -0.016765648499131203,\n",
       " 0.017619017511606216,\n",
       " 0.09507094323635101,\n",
       " 0.02456720732152462,\n",
       " -0.00030371572938747704,\n",
       " 0.07726727426052094,\n",
       " 0.009286770597100258,\n",
       " 0.04579121246933937,\n",
       " 0.0006386045133695006,\n",
       " -0.0203663669526577,\n",
       " 0.05929982289671898,\n",
       " -0.009882405400276184,\n",
       " 0.017564021050930023,\n",
       " -0.05515163019299507,\n",
       " 0.016189292073249817,\n",
       " 0.028766026720404625,\n",
       " -0.03514404594898224,\n",
       " -0.04935841262340546,\n",
       " -0.02447367087006569,\n",
       " -0.02685004286468029,\n",
       " -0.009837007150053978,\n",
       " 9.710079029900953e-05,\n",
       " 0.019255781546235085,\n",
       " 0.016495689749717712,\n",
       " 0.019392136484384537,\n",
       " 0.014026164077222347,\n",
       " 0.044185783714056015,\n",
       " -0.0408172607421875,\n",
       " 0.058831170201301575,\n",
       " 0.033452264964580536,\n",
       " -0.07100701332092285,\n",
       " -0.01207928266376257,\n",
       " 0.017947617918252945,\n",
       " 0.026635218411684036,\n",
       " -0.042489077895879745,\n",
       " -0.025743063539266586,\n",
       " 0.04628248140215874,\n",
       " 0.02357054501771927,\n",
       " 0.012330091558396816,\n",
       " -0.019114600494503975,\n",
       " 0.04284172132611275,\n",
       " 0.020867055281996727,\n",
       " -0.020985012874007225,\n",
       " 0.049807094037532806,\n",
       " -0.06530244648456573,\n",
       " 0.06784137338399887,\n",
       " 0.06132720038294792,\n",
       " -0.025619087740778923,\n",
       " -0.01975896954536438,\n",
       " -0.028557993471622467,\n",
       " -0.0033664510119706392,\n",
       " -0.022265056148171425,\n",
       " 0.0277670007199049,\n",
       " 0.0376681424677372,\n",
       " -0.01844070851802826,\n",
       " -0.0666554793715477,\n",
       " -0.03576522693037987,\n",
       " -0.006980367470532656,\n",
       " -0.009456862695515156,\n",
       " 0.007616228424012661,\n",
       " -0.002117250580340624,\n",
       " -0.000508638215251267,\n",
       " -0.058485377579927444,\n",
       " 0.02196097932755947,\n",
       " 0.011461308225989342,\n",
       " -0.019124537706375122,\n",
       " 0.017011091113090515,\n",
       " -0.03894034028053284,\n",
       " -0.024677084758877754,\n",
       " -0.009032917208969593,\n",
       " 0.019948668777942657,\n",
       " -0.020086267963051796,\n",
       " 0.0046862466260790825,\n",
       " 0.0658910721540451,\n",
       " -0.021722840145230293,\n",
       " -0.0028560664504766464,\n",
       " 0.011400393210351467,\n",
       " -0.014010073617100716,\n",
       " -0.05337538197636604,\n",
       " -0.07439275830984116,\n",
       " 0.0002861053217202425,\n",
       " 0.04116436839103699,\n",
       " 0.03818747028708458,\n",
       " -0.012812647968530655,\n",
       " 0.05230933055281639,\n",
       " 0.018940966576337814,\n",
       " -0.048808012157678604,\n",
       " -0.06779120117425919,\n",
       " -0.01911095529794693,\n",
       " -0.028975164517760277,\n",
       " 0.004133264068514109,\n",
       " 0.01923450082540512,\n",
       " -0.020669246092438698,\n",
       " -0.003695683553814888,\n",
       " 0.009470553137362003,\n",
       " -0.041596852242946625,\n",
       " 0.045980364084243774,\n",
       " 0.029285280033946037,\n",
       " -0.07706877589225769,\n",
       " 0.006656208075582981,\n",
       " -0.005012008361518383,\n",
       " -0.017416084185242653,\n",
       " 0.007063459139317274,\n",
       " -0.05976015701889992,\n",
       " 0.0185412485152483,\n",
       " -0.0273988489061594,\n",
       " 0.005803277250379324,\n",
       " -0.041785452514886856,\n",
       " -0.0908936932682991,\n",
       " -0.006833197548985481,\n",
       " -0.014357824809849262,\n",
       " 0.08537802845239639,\n",
       " -0.05669703334569931,\n",
       " 0.06276111304759979,\n",
       " 0.0009012018563225865,\n",
       " -0.007769424468278885,\n",
       " -0.02144297957420349,\n",
       " -0.11318439245223999,\n",
       " 0.07541830092668533,\n",
       " 0.023151574656367302,\n",
       " 0.005866213236004114,\n",
       " -0.004821085371077061,\n",
       " -0.010738340206444263,\n",
       " 0.02703235298395157,\n",
       " 0.025599531829357147,\n",
       " 0.007024332880973816,\n",
       " -0.032584112137556076,\n",
       " -0.04185543581843376,\n",
       " -0.024687552824616432,\n",
       " -0.02051878534257412,\n",
       " -0.04894666746258736,\n",
       " 0.03619479760527611,\n",
       " -0.04281015321612358,\n",
       " 0.014844655990600586,\n",
       " 0.010251615196466446,\n",
       " 0.023320626467466354,\n",
       " 0.01871577650308609,\n",
       " 0.03378577157855034,\n",
       " -0.025527965277433395,\n",
       " 0.044417854398489,\n",
       " -0.02600604109466076,\n",
       " -0.0119530213996768,\n",
       " -0.055049628019332886,\n",
       " 0.022267477586865425,\n",
       " -0.010707407258450985,\n",
       " -0.026371469721198082,\n",
       " -0.009768350049853325,\n",
       " -0.01535708550363779,\n",
       " -0.019579773768782616,\n",
       " -0.021421188488602638,\n",
       " 0.006235864479094744,\n",
       " 0.0314040407538414,\n",
       " 0.031010085716843605,\n",
       " 0.004555661231279373,\n",
       " -0.031087100505828857,\n",
       " -0.014605932869017124,\n",
       " -0.003368874778971076,\n",
       " -0.028359955176711082,\n",
       " 0.057969674468040466,\n",
       " -0.09270115196704865,\n",
       " -0.009005305357277393,\n",
       " 0.022297324612736702,\n",
       " -0.02122938074171543,\n",
       " -0.03563996031880379,\n",
       " 0.004106925334781408,\n",
       " 0.006597382482141256,\n",
       " 0.03264277055859566,\n",
       " 0.03904365748167038,\n",
       " 0.02032073214650154,\n",
       " 0.005939505994319916,\n",
       " 0.010560653172433376,\n",
       " -0.002243859926238656,\n",
       " 0.02130184881389141,\n",
       " -0.02763022854924202,\n",
       " 0.014897515065968037,\n",
       " -0.020667249336838722,\n",
       " -0.09472247958183289,\n",
       " -0.0004307603812776506,\n",
       " -0.02479407750070095,\n",
       " 0.028932971879839897,\n",
       " 0.025980545207858086,\n",
       " 0.057643499225378036,\n",
       " -0.0374685563147068,\n",
       " 0.0006257054628804326,\n",
       " -0.01778700202703476,\n",
       " 0.017848286777734756,\n",
       " -0.00700916163623333,\n",
       " -0.026394061744213104,\n",
       " 0.03204090893268585,\n",
       " -0.005773015785962343,\n",
       " -0.012837935239076614,\n",
       " 0.016041608527302742,\n",
       " 0.030365966260433197,\n",
       " -0.025132445618510246,\n",
       " 0.0380871519446373,\n",
       " 0.026565272361040115,\n",
       " 0.061460208147764206,\n",
       " 0.018262824043631554,\n",
       " -0.025590986013412476,\n",
       " -0.014801396057009697,\n",
       " -0.009373162873089314,\n",
       " -0.05065552145242691,\n",
       " -0.01938270404934883,\n",
       " 0.04338742420077324,\n",
       " -0.008704069070518017,\n",
       " 0.020551657304167747,\n",
       " 0.010078946128487587,\n",
       " -0.022332625463604927,\n",
       " 0.013624574989080429,\n",
       " -0.029078511521220207,\n",
       " -0.02031247317790985,\n",
       " 0.04527844116091728,\n",
       " 0.008548842743039131,\n",
       " -0.044655926525592804,\n",
       " -0.04993825778365135,\n",
       " -0.0005616651615127921,\n",
       " -0.0019093779847025871,\n",
       " 0.029475873336195946,\n",
       " 0.08492345362901688,\n",
       " 0.02737259492278099,\n",
       " -0.019202347844839096,\n",
       " -0.011180019937455654,\n",
       " 0.0613800473511219,\n",
       " 0.0009942551841959357,\n",
       " -0.005685679614543915,\n",
       " 0.010302840732038021,\n",
       " 0.0048909238539636135,\n",
       " 0.03689644858241081,\n",
       " 6.452776142396033e-05,\n",
       " -0.02497044950723648,\n",
       " 0.006457015406340361,\n",
       " -0.03664189949631691,\n",
       " -0.03386320546269417,\n",
       " -0.009320611134171486,\n",
       " 0.027527937665581703,\n",
       " -0.006957217585295439,\n",
       " -0.0016937933396548033,\n",
       " 0.02136092633008957,\n",
       " 0.008775141090154648,\n",
       " 0.031389400362968445,\n",
       " 0.05911479890346527,\n",
       " 0.10196645557880402,\n",
       " 0.03833167254924774,\n",
       " 0.059265561401844025,\n",
       " -0.034477267414331436,\n",
       " 0.01983230747282505,\n",
       " -0.028979182243347168,\n",
       " -0.0006717467331327498,\n",
       " 0.023720132187008858,\n",
       " 0.009949897415935993,\n",
       " -0.027036644518375397,\n",
       " -0.02788533642888069,\n",
       " -0.009638091549277306,\n",
       " -0.017497442662715912,\n",
       " 0.05965723469853401,\n",
       " -0.023421915248036385,\n",
       " -7.17904549674131e-05,\n",
       " -0.023910334333777428,\n",
       " 0.010167197324335575,\n",
       " 0.06294538080692291,\n",
       " -0.007855839096009731,\n",
       " 0.009582506492733955,\n",
       " -0.00771779241040349,\n",
       " 0.010753695853054523,\n",
       " 0.029988912865519524,\n",
       " -0.0410747155547142,\n",
       " 0.014009366743266582,\n",
       " -0.016409873962402344,\n",
       " -0.025754181668162346,\n",
       " -0.029275324195623398,\n",
       " 0.06891670823097229,\n",
       " 0.00874270685017109,\n",
       " 0.023196490481495857,\n",
       " -0.038228556513786316,\n",
       " 0.016259821131825447,\n",
       " -0.013566256500780582,\n",
       " 0.025782182812690735,\n",
       " -0.020683975890278816,\n",
       " 0.04356987029314041,\n",
       " 0.03843095898628235,\n",
       " 0.011759771965444088,\n",
       " 0.0006424587918445468,\n",
       " 0.06934863328933716,\n",
       " 0.004451766610145569,\n",
       " 0.05257083848118782,\n",
       " 0.05299004167318344,\n",
       " -0.027428876608610153,\n",
       " 0.007362625561654568,\n",
       " -0.027196450158953667,\n",
       " -0.03716901317238808,\n",
       " -0.05996355041861534,\n",
       " -0.018047159537672997,\n",
       " 0.010596984066069126,\n",
       " -0.019895626232028008,\n",
       " -0.02837696112692356,\n",
       " -0.02666792832314968,\n",
       " 0.005059539806097746,\n",
       " -0.00711184972897172,\n",
       " 0.028563370928168297,\n",
       " 0.10317031294107437,\n",
       " 0.04524599015712738,\n",
       " -0.09671919792890549,\n",
       " -0.05298631638288498,\n",
       " -0.02709105610847473,\n",
       " -0.03117411397397518,\n",
       " -0.010037784464657307,\n",
       " 0.0028370970394462347,\n",
       " -0.01989951729774475,\n",
       " 0.054531022906303406,\n",
       " 0.017425885424017906,\n",
       " -0.031392212957143784,\n",
       " -0.0492844320833683,\n",
       " 0.012391205877065659,\n",
       " 0.021212412044405937,\n",
       " -0.010904794558882713,\n",
       " -0.02079067938029766,\n",
       " 0.010785725899040699,\n",
       " 0.0042291851714253426,\n",
       " 0.007526397705078125,\n",
       " 0.018077723681926727,\n",
       " -0.030186735093593597,\n",
       " -0.06504229456186295,\n",
       " -0.0060129123739898205,\n",
       " 0.04470229893922806,\n",
       " -0.0001683917362242937,\n",
       " 0.014180039055645466,\n",
       " 0.02072112262248993,\n",
       " 0.001374627579934895,\n",
       " 0.03986472263932228,\n",
       " 0.02338232658803463,\n",
       " 0.0028986725956201553,\n",
       " 0.018854621797800064,\n",
       " 0.018174119293689728,\n",
       " 0.008018970489501953,\n",
       " -0.001832504291087389,\n",
       " 0.004592920653522015,\n",
       " -0.01788954995572567,\n",
       " 0.004617960192263126,\n",
       " -0.028025709092617035,\n",
       " 0.04705401882529259,\n",
       " 0.027661263942718506,\n",
       " 0.0065150074660778046,\n",
       " -0.030028309673070908,\n",
       " -0.05390259250998497,\n",
       " -0.0013852387201040983,\n",
       " -0.01643216609954834,\n",
       " -0.06801334023475647,\n",
       " 0.05069683864712715,\n",
       " 0.06540688872337341,\n",
       " 0.01848502829670906,\n",
       " 0.03506237268447876,\n",
       " 0.015128708444535732,\n",
       " -0.0009853191440925002,\n",
       " 0.018667006865143776,\n",
       " -0.02371547929942608,\n",
       " -0.03277934715151787,\n",
       " -0.04126175865530968,\n",
       " 0.0004407509695738554,\n",
       " -0.006913974415510893,\n",
       " 0.001071878825314343,\n",
       " 0.04121888801455498,\n",
       " 0.033742137253284454,\n",
       " 0.06251147389411926,\n",
       " 0.07336639612913132,\n",
       " 0.036337900906801224,\n",
       " -0.020660797134041786,\n",
       " -0.023938000202178955,\n",
       " 0.02654903009533882,\n",
       " -0.009692799299955368,\n",
       " -0.04504105821251869,\n",
       " 0.022294068709015846,\n",
       " 0.03183344379067421,\n",
       " -0.013615928590297699,\n",
       " 0.09686916321516037,\n",
       " 0.05850844457745552,\n",
       " 0.05051637440919876,\n",
       " 0.06556273996829987,\n",
       " -0.030124453827738762,\n",
       " -0.05741645395755768,\n",
       " 0.08546410501003265,\n",
       " -0.09430325776338577,\n",
       " -0.0328102707862854,\n",
       " -0.026789216324687004,\n",
       " 0.03717803210020065,\n",
       " 0.08715295791625977,\n",
       " 0.0001604699791641906,\n",
       " -0.009505724534392357,\n",
       " -0.01792062073945999,\n",
       " 0.008379129692912102,\n",
       " 0.0796549916267395,\n",
       " 0.033412281423807144,\n",
       " 0.04523250833153725,\n",
       " 0.002415842143818736,\n",
       " -0.08875352889299393,\n",
       " -0.02112312614917755,\n",
       " 0.034831877797842026,\n",
       " 0.008336428552865982,\n",
       " 0.03964807465672493,\n",
       " 0.022455615922808647,\n",
       " 0.010564827360212803,\n",
       " -0.02183358184993267,\n",
       " -0.03671906143426895,\n",
       " 0.016374699771404266,\n",
       " -0.07664699852466583,\n",
       " -0.04381590336561203,\n",
       " 0.008805062621831894,\n",
       " -0.029425987973809242,\n",
       " 0.035350218415260315,\n",
       " 0.03271523490548134,\n",
       " -0.008388573303818703,\n",
       " -0.021349893882870674,\n",
       " 0.01584000512957573,\n",
       " -0.014425228349864483,\n",
       " -0.001280902768485248,\n",
       " -0.015345528721809387,\n",
       " -0.05014083534479141,\n",
       " 0.004639973398298025,\n",
       " 0.00799761712551117,\n",
       " 0.01279925275593996,\n",
       " 0.01869218423962593,\n",
       " 0.050975508987903595,\n",
       " -0.004513971973210573]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.embed_query(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0a81a",
   "metadata": {},
   "source": [
    "# 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6b1f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9531122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6f30b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\portfolio projects details\\\\LLMOPs_Projects\\\\document_portal_project\\\\notebook'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b141ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), \"data\", \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb564323",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader =PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49472c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88e7e9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5945ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a experimental value. There is no deterministic way to split the text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150,length_function=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f9bf39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99a18baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ee282c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55ed3042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We found this margin component can improve Helpfulness reward model accuracy especially on samples\\nwhere two responses are more separable. More detailed ablation and analysis can be found in Table 28 in\\nAppendix A.3.3.\\nData Composition. We combine our newly collected data with existing open-source preference datasets\\nto form a larger training dataset. Initially, open-source datasets were used to bootstrap our reward models'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[100].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f328ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06b40118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.embed_documents(docs[0].page_content)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50fc834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d890a",
   "metadata": {},
   "source": [
    "This above data is in-memory and we need to transfer it to the on disk storage.\n",
    "Faiss is in-memory vector store and we can also persist it over the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e10148b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc= vectorstore.similarity_search(\"llama2 finetuning benchmark experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9267893f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc= vectorstore.similarity_search(\"llama2 finetuning benchmark experiments.\",k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7df46e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be on par with some of the closed-source models, at least on the human evaluations we performed (see\\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[9].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b352d",
   "metadata": {},
   "source": [
    "We need to explore about the keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0007d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e01dcc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f773e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5478dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1d5c393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n        Answer the question based on the context provided below. \\n        If the context does not contain sufficient information, respond with: \\n        \"I do not have enough information about this.\"\\n\\n        Context: {context}\\n\\n        Question: {question}\\n\\n        Answer:')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11195903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "785129dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a9fd134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e3a7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df665235",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\":retriever | format_docs, \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "049b6fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so the user is asking about the Llama 2 fine-tuning benchmark experiments. I need to look through the provided context to find relevant information. \\n\\nFirst, I\\'ll scan through the context. There are several tables and sections. Table 3 mentions overall performance on academic benchmarks, including Llama 2 models with different parameter sizes (7B, 13B, 34B, 70B). The results are listed, but it\\'s not clear if these are fine-tuned or not. \\n\\nLooking further, I see a section that mentions \"Fine-tuned\" with ChatGPT and other models like MPT-instruct and Falcon-instruct. There are some numbers, but it\\'s a bit confusing. There\\'s also a part about ablation experiments and hyperparameters, but that\\'s more about training than fine-tuning benchmarks.\\n\\nAnother table shows percentages for true, info, and combined scores for various models, including fine-tuned versions of Llama 2. For example, ChatGPT has high scores, and Llama 2-Chat models also have good results. \\n\\nHowever, the context doesn\\'t provide detailed explanations of the fine-tuning process, the datasets used, or specific benchmarks beyond the tables. It mentions that the results are from evaluations, but without more context, it\\'s hard to give a comprehensive answer.\\n\\nSo, I realize that while there are some benchmark results for fine-tuned Llama 2 models, the context lacks detailed information about the experiments themselves, like the setup, parameters, or specific tasks used. Therefore, I can\\'t provide a complete overview based on the given information.\\n</think>\\n\\nI do not have enough information about this.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tell me about the llama2 finetuning benchmark experiments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "087f9d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I need to answer the question: \"can you tell me the granular reward model accuracy per preference\". Let me look through the provided context to find the relevant information.\\n\\nFirst, I\\'ll scan the context for keywords like \"granular\", \"reward model accuracy\", and \"preference\". I notice a section titled \"Granular reward model accuracy per preference rating.\" That sounds promising.\\n\\nLooking closer, there\\'s a table mentioned, Table 8, which reports per-preference rating accuracy for both Helpfulness and Safety reward models. The table includes metrics like Safety RM, Meta Safety, Helpfulness RM, and others across different preference ratings.\\n\\nI also see that the context discusses how the accuracy varies with the preference ratings. For instance, the models show higher accuracy on more distinct responses (like \"significantly better\") and lower accuracy on similar ones (like \"negligibly better\"). This suggests that the granular accuracy depends on how distinct the preferences are.\\n\\nSo, putting this together, the context does provide specific granular accuracy data in Table 8, which breaks down the accuracy per preference rating for both Helpfulness and Safety reward models.\\n</think>\\n\\nThe granular reward model accuracy per preference rating is detailed in Table 8 of the context. This table provides the accuracy for both Helpfulness and Safety reward models across different preference ratings, showing higher accuracy for more distinct preferences and lower accuracy for similar ones.\\n\\n**Answer:** The granular reward model accuracy per preference rating is presented in Table 8, which shows the accuracy for both Helpfulness and Safety reward models across different ratings, with higher accuracy for more distinct preferences and lower for similar ones.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"can you tell me the granular reward model accuracy per preference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa8455a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, so I need to figure out the scaling trends for the reward model based on the provided context. Let me go through the context step by step to find the relevant information.\\n\\nFirst, I see a mention of \"Figure 6\" which reports the scaling trends. The context says that larger models achieve higher performance with a similar volume of data. That suggests that as the model size increases, the performance improves, which is a positive scaling trend.\\n\\nIt also mentions that the scaling performance hasn\\'t plateaued yet, meaning there\\'s still room for improvement as more data is annotated. This indicates that adding more training data could lead to better performance, which is another positive sign for scaling.\\n\\nI don\\'t see any negative trends mentioned here. The focus is on how larger models and more data contribute to better performance. The context doesn\\'t discuss any limitations or diminishing returns, so it seems the scaling is effective so far.\\n\\nSo, putting it together, the scaling trends for the reward model show that larger models perform better with the same amount of data, and there\\'s potential for further improvement with more data.\\n</think>\\n\\nThe scaling trends for the reward model, as reported in Figure 6, indicate that larger models achieve higher performance with a similar volume of data. Additionally, the scaling performance has not yet plateaued, suggesting there is room for further improvement with more data annotations. This implies that increasing model size and data volume positively impacts the reward model\\'s effectiveness.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"can you tell me the scaling trends for the reward model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3323ac",
   "metadata": {},
   "source": [
    "One Small task:\n",
    "\n",
    "Take 10 pdfs keep it in same directory and create RAG on top of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220e8a9",
   "metadata": {},
   "source": [
    "Testing token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf76312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"available_keys\": [\"GOOGLE_API_KEY\", \"GROQ_API_KEY\"], \"timestamp\": \"2025-08-03T19:06:27.217477Z\", \"level\": \"info\", \"event\": \"Environment variables validated\"}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config\\\\config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelLoader\n\u001b[1;32m----> 2\u001b[0m loader\u001b[38;5;241m=\u001b[39m\u001b[43mModelLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\portfolio projects details\\llmops_projects\\document_portal\\utils\\model_loader.py:24\u001b[0m, in \u001b[0;36mModelLoader.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m load_dotenv()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_env()\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m=\u001b[39m\u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfiguration loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m, config_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "File \u001b[1;32md:\\portfolio projects details\\llmops_projects\\document_portal\\utils\\config_loader.py:5\u001b[0m, in \u001b[0;36mload_config\u001b[1;34m(config_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_config\u001b[39m(config_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m         config\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39msafe_load(file)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config\\\\config.yaml'"
     ]
    }
   ],
   "source": [
    "from utils.model_loader import ModelLoader\n",
    "loader=ModelLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68256c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e77d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6821086b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m\u001b[43mloader\u001b[49m\u001b[38;5;241m.\u001b[39mload_llm()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "llm =loader.load_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b57766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 691\n",
      "\tPrompt Tokens: 12\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 679\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"Tell me a joke\")\n",
    "    print(cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fc0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
